{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install compatible TensorFlow and tensorflow-privacy(sol)\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall tensorflow-privacy -y \n",
    "# !pip install tensorflow==2.14.0\n",
    "# !pip install tensorflow-privacy==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-estimator==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Standard Library\n",
    "# =========================\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# =========================\n",
    "# Third-Party: Numerical / Data\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Third-Party: Visualization\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================\n",
    "# Third-Party: sklearn\n",
    "# =========================\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Third-Party: Imbalanced-Learn\n",
    "# =========================\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# =========================\n",
    "# Third-Party: TensorFlow & Privacy\n",
    "# =========================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import (\n",
    "    TruePositives,\n",
    "    TrueNegatives,\n",
    "    FalsePositives,\n",
    "    FalseNegatives,\n",
    ")\n",
    "\n",
    "\n",
    "# !pip install tensorflow-privacy==0.8.0\n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "# =========================\n",
    "# Third-Party: Genetic Algorithm (DEAP)\n",
    "# =========================\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# =========================\n",
    "# Warnings\n",
    "# =========================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y4yw8j0dm7A"
   },
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50K_iMkJdmPG"
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv(r\"C:\\Users\\dmachooka\\Downloads\\dmac\\IoT_Intrusion.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "6jAJqFM_dy7t",
    "outputId": "30612475-f51e-4be4-ed40-53047710ece6"
   },
   "outputs": [],
   "source": [
    "# Multiclass problem\n",
    "target_column = 'label'\n",
    "display(data[target_column].unique())\n",
    "num_classes = data[target_column].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1rzDuTFScNXA",
    "outputId": "ca321bc3-0a3e-485a-a5be-c5c7586cc6c9"
   },
   "outputs": [],
   "source": [
    "data[target_column].value_counts() # Imbalance learning not feasible, simplify the problem lets only predict for top 3 conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R83V6cLQcTAK"
   },
   "outputs": [],
   "source": [
    "# simplify the problem lets only predict for top 3 conditions\n",
    "data = data[data[target_column].isin(['DDoS-ICMP_Flood','DDoS-UDP_Flood','DDoS-TCP_Flood'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-lyE57xd8_X"
   },
   "outputs": [],
   "source": [
    "# Label encode the target column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target column\n",
    "data[target_column] = label_encoder.fit_transform(data[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpQ9SnVGeJWq",
    "outputId": "4b2e6b00-0ef1-4746-b3fe-8d734921661e"
   },
   "outputs": [],
   "source": [
    "# Identify non-numeric columns (there are none)\n",
    "print(data.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPpDWJFneNHz"
   },
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in data.columns if col not in [target_column]] # Other columns\n",
    "\n",
    "X = data.drop(target_column, axis=1)\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into training test and validation datasets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Perfom feature scaling for the training set\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[categorical_columns] = scaler.fit_transform(X_train[categorical_columns])\n",
    "\n",
    "#Perfom feature scaling for the training set\n",
    "X_valid_scaled = X_valid.copy()\n",
    "X_valid_scaled[categorical_columns] = scaler.transform(X_valid[categorical_columns])\n",
    "\n",
    "#Perfom feature scaling for the test set\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[categorical_columns] = scaler.transform(X_test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMmDVlGW7II6"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(history1, history2=None):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training & validation loss values for the first model\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history1.history['loss'])\n",
    "    plt.plot(history1.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    # Plot training & validation accuracy values for the first model\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history1.history['accuracy'])\n",
    "    plt.plot(history1.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "    if history2 is not None:\n",
    "        # Plot training & validation loss values for the second model\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history2.history['loss'], linestyle='dashed')\n",
    "        plt.plot(history2.history['val_loss'], linestyle='dashed')\n",
    "        plt.legend(['Train', 'Validation', 'Train Privacy', 'Validation Privacy'], loc='upper right')\n",
    "\n",
    "        # Plot training & validation accuracy values for the second model\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history2.history['accuracy'], linestyle='dashed')\n",
    "        plt.plot(history2.history['val_accuracy'], linestyle='dashed')\n",
    "        plt.legend(['Train', 'Validation', 'Train Privacy', 'Validation Privacy'], loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l3pyG_Pe0Fj"
   },
   "source": [
    "## GA Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVEctWtd2llt"
   },
   "outputs": [],
   "source": [
    "# --- Define parameter search space ---\n",
    "LSTM_UNITS = [16, 32, 64, 128, 256]\n",
    "DENSE_UNITS = [16, 32, 64, 128, 256]\n",
    "ACTIVATIONS = ['gelu', 'swish', 'silu', 'relu']\n",
    "OPTIMIZERS = ['sgd', 'adam']\n",
    "LOSSES = ['sparse_categorical_crossentropy']\n",
    "LEARNING_RATES = [0.001, 0.0001]\n",
    "BATCH_SIZES = [64, 128, 256]\n",
    "EPOCHS = [20, 30, 50, 100]\n",
    "MAX_LSTM_LAYERS = 3\n",
    "MAX_DENSE_LAYERS = 4\n",
    "noise_multiplier=1.3\n",
    "gene_alg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BUmZ4zc6pUi"
   },
   "outputs": [],
   "source": [
    "def evaluate_nn_dp(hyperparams):\n",
    "    # Unpack hyperparameters\n",
    "    lstm_units = hyperparams['lstm_units']\n",
    "    dense_units = hyperparams['dense_units']\n",
    "    activation = hyperparams['activation']\n",
    "    optimizer_choice = hyperparams['optimizer']\n",
    "    loss_choice = hyperparams['loss']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    epochs = hyperparams['epochs']\n",
    "    dropouts = hyperparams['dropouts']\n",
    "    l2_norm_clip = hyperparams['l2_norm_clip']\n",
    "    noise_multiplier = hyperparams['noise_multiplier']\n",
    "\n",
    "    # Define sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input LSTM layers\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_sequences = True if i < len(lstm_units) - 1 else False\n",
    "        model.add(LSTM(units=units, activation=activation,\n",
    "                       return_sequences=return_sequences,\n",
    "                       input_shape=(1,len(categorical_columns))))\n",
    "        model.add(Dropout(dropouts[i]))\n",
    "\n",
    "    # Dense layers\n",
    "    for j, units in enumerate(dense_units):\n",
    "        model.add(Dense(units=units, activation=activation))\n",
    "        # can remove len(lstm_units)+ make it equall\n",
    "        model.add(Dropout(dropouts[len(lstm_units) + j])) \n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # DP Optimizer\n",
    "    if optimizer_choice == \"adam\":\n",
    "        base_optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        base_optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    dp_optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        num_microbatches=1,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=dp_optimizer, loss=loss_choice, metrics=['accuracy'])\n",
    "\n",
    "    # Perform Stratified K-Fold CV\n",
    "    kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    cv_results = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train_scaled[categorical_columns], y_train):\n",
    "        X_train_fold, X_val_fold = (\n",
    "            X_train_scaled[categorical_columns].iloc[train_index],\n",
    "            X_train_scaled[categorical_columns].iloc[test_index]\n",
    "        )\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Reshape for LSTM: (samples, timesteps=1, features)\n",
    "        X_train_fold = np.expand_dims(X_train_fold, axis=1)\n",
    "        X_val_fold = np.expand_dims(X_val_fold, axis=1)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        # _, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        # cv_results.append(val_accuracy)\n",
    "        # Predict class labels for F1\n",
    "        y_pred_proba = model.predict(X_val_fold, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)   # convert softmax â†’ class\n",
    "\n",
    "        # Compute F1\n",
    "        # f1 = f1_score(y_val_fold, y_pred, average=\"binary\")  # or \"macro\", \"weighted\"\n",
    "        f1 = f1_score(y_val_fold, y_pred, average=\"macro\")\n",
    "        cv_results.append(f1)\n",
    "\n",
    "    # Extract trainable variables from the model\n",
    "    trainable_variables = model.trainable_variables\n",
    "\n",
    "    # Compute privacy budget\n",
    "    privacy_report = compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(\n",
    "        number_of_examples=len(X_train_scaled),\n",
    "        batch_size=batch_size,\n",
    "        noise_multiplier=1.3,\n",
    "        num_epochs=epochs,\n",
    "        delta=1e-5\n",
    "    )\n",
    "\n",
    "    epsilon = None\n",
    "\n",
    "    # Case A: report is an object with epsilon attr\n",
    "    if hasattr(privacy_report, \"epsilon\"):\n",
    "        epsilon = privacy_report.epsilon\n",
    "\n",
    "    # Case B: report is a tuple (epsilon, something)\n",
    "    elif isinstance(privacy_report, tuple) and len(privacy_report) >= 1 and isinstance(privacy_report[0], (float, int)):\n",
    "        epsilon = privacy_report[0]\n",
    "\n",
    "    # Case C: report is a string\n",
    "    elif isinstance(privacy_report, str):\n",
    "        import re\n",
    "        match = re.search(r\"epsilon[^0-9]*([0-9.]+)\", privacy_report.lower())\n",
    "        if match:\n",
    "            epsilon = float(match.group(1))\n",
    "\n",
    "    score = np.mean(cv_results) - epsilon # GA maximizes score by default\n",
    "\n",
    "    return score,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVNGyDX_It3X"
   },
   "outputs": [],
   "source": [
    "def custom_crossover(ind1, ind2):\n",
    "    keys = list(ind1.keys())\n",
    "    cxpoint1, cxpoint2 = sorted([random.randint(1, len(keys)-1) for _ in range(2)])\n",
    "    for i in range(cxpoint1, cxpoint2):\n",
    "        key = keys[i]\n",
    "        ind1[key], ind2[key] = ind2[key], ind1[key]\n",
    "    return ind1, ind2\n",
    "\n",
    "def custom_mutation(ind):\n",
    "    param_to_mutate = random.choice(list(ind.keys()))\n",
    "\n",
    "    if param_to_mutate == 'lstm_units':\n",
    "        ind['lstm_units'] = [random.choice(LSTM_UNITS) for _ in range(len(ind['lstm_units']))]\n",
    "    elif param_to_mutate == 'lstm_dropouts':\n",
    "        ind['lstm_dropouts'] = [random.uniform(0.1, 0.5) for _ in range(len(ind['lstm_dropouts']))]\n",
    "    elif param_to_mutate == 'dense_units':\n",
    "        ind['dense_units'] = [random.choice(DENSE_UNITS) for _ in range(len(ind['dense_units']))]\n",
    "    elif param_to_mutate == 'dense_dropouts':\n",
    "        ind['dense_dropouts'] = [random.uniform(0.1, 0.5) for _ in range(len(ind['dense_dropouts']))]\n",
    "    elif param_to_mutate == 'activation':\n",
    "        ind['activation'] = random.choice(ACTIVATIONS)\n",
    "    elif param_to_mutate == 'optimizer':\n",
    "        ind['optimizer'] = random.choice(OPTIMIZERS)\n",
    "    elif param_to_mutate == 'loss':\n",
    "        ind['loss'] = random.choice(LOSSES)\n",
    "    elif param_to_mutate == 'learning_rate':\n",
    "        ind['learning_rate'] = random.choice(LEARNING_RATES)\n",
    "    elif param_to_mutate == 'batch_size':\n",
    "        ind['batch_size'] = random.choice(BATCH_SIZES)\n",
    "    elif param_to_mutate == 'epochs':\n",
    "        ind['epochs'] = random.choice(EPOCHS)\n",
    "\n",
    "    return ind,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER0j3wl86qIq",
    "outputId": "5377d05f-c6ae-46e7-f5fe-741a6e935249"
   },
   "outputs": [],
   "source": [
    "def create_hyperparameter_set_dp():\n",
    "    # Random number of LSTM and Dense layers\n",
    "    n_lstm = random.randint(1, MAX_LSTM_LAYERS)\n",
    "    n_dense = random.randint(1, MAX_DENSE_LAYERS)\n",
    "\n",
    "    return {\n",
    "        'lstm_units': [random.choice(LSTM_UNITS) for _ in range(n_lstm)],\n",
    "        'dense_units': [random.choice(DENSE_UNITS) for _ in range(n_dense)],\n",
    "        'activation': random.choice(ACTIVATIONS),\n",
    "        'optimizer': random.choice(OPTIMIZERS),\n",
    "        'loss': random.choice(LOSSES),\n",
    "        'learning_rate': random.choice(LEARNING_RATES),\n",
    "        'batch_size': random.choice(BATCH_SIZES),\n",
    "        'epochs': random.choice(EPOCHS),\n",
    "        'dropouts': [random.uniform(0.1, 0.5) for _ in range(n_lstm + n_dense)],\n",
    "        'l2_norm_clip': random.uniform(0.1, 5.0),\n",
    "        # 'noise_multiplier': random.uniform(0.5, 2.0) has been hard coded 1.3 at top\n",
    "        'noise_multiplier': random.uniform(1.3, 1.3)\n",
    "    }\n",
    "\n",
    "\n",
    "# === DEAP Setup ===\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", dict, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_hyperparameter_set_dp)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# --- NOTE: custom crossover must be defined ---\n",
    "toolbox.register(\"mate\", custom_crossover)\n",
    "\n",
    "# Mutate: random re-sample of new hyperparams\n",
    "def custom_mutate(individual):\n",
    "    new_params = create_hyperparameter_set_dp()\n",
    "    individual.update(new_params)\n",
    "    return individual,\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate_nn_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZYcM_s46sMB"
   },
   "outputs": [],
   "source": [
    "def genetic_algorithm_dp():\n",
    "    pop = toolbox.population(n=10)\n",
    "    ngen = 5  # Number of generations\n",
    "    cxpb, mutpb = 0.5, 0.2  # Crossover and mutation probabilities\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        print(f\"-- Generation {gen} --\")\n",
    "\n",
    "        # Evaluate all individuals\n",
    "        fitnesses = list(map(toolbox.evaluate, pop))\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Select individuals for the next generation\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < cxpb:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutpb:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        pop[:] = offspring\n",
    "\n",
    "    # Return the best individual\n",
    "    top_individual = tools.selBest(pop, 1)[0]\n",
    "    return top_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debuging\n",
    "# dir(best_hyperparameters_dp = genetic_algorithm_dp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF0h2a0g6vT4",
    "outputId": "28014041-075f-4c7e-9168-1ff181e3854e"
   },
   "outputs": [],
   "source": [
    "# Run the genetic algorithm\n",
    "gene_alg = True\n",
    "if gene_alg:\n",
    "  best_hyperparameters_dp = genetic_algorithm_dp()\n",
    "  print(\"Best Hyperparameters with DP:\", best_hyperparameters_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Qlk_GOZ26w7R",
    "outputId": "7960ec57-691a-40a9-e52f-6b149d7419ed"
   },
   "outputs": [],
   "source": [
    "if gene_alg:\n",
    "    best_hp_dp = {\n",
    "        'lstm_units': best_hyperparameters_dp['lstm_units'],\n",
    "        'dense_units': best_hyperparameters_dp['dense_units'],\n",
    "        'activation': best_hyperparameters_dp['activation'],\n",
    "        'optimizer': best_hyperparameters_dp['optimizer'],\n",
    "        'loss': best_hyperparameters_dp['loss'],\n",
    "        'learning_rate': best_hyperparameters_dp['learning_rate'],\n",
    "        'batch_size': best_hyperparameters_dp['batch_size'],\n",
    "        'epochs': best_hyperparameters_dp['epochs'],\n",
    "        'dropouts': best_hyperparameters_dp['dropouts'],\n",
    "        'l2_norm_clip': best_hyperparameters_dp['l2_norm_clip'],\n",
    "        'noise_multiplier': best_hyperparameters_dp['noise_multiplier']\n",
    "    }\n",
    "else:\n",
    "    best_hp_dp = {\n",
    "        'lstm_units': [64, 128],   # Example manual config\n",
    "        'dense_units': [256, 128], # Example manual config\n",
    "        'activation': 'relu',\n",
    "        'optimizer': 'sgd',\n",
    "        'loss': 'sparse_categorical_crossentropy',\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 30,\n",
    "        'dropouts': [0.2, 0.3, 0.25, 0.2],  # one per layer (LSTM + Dense)\n",
    "        'l2_norm_clip': 1.0,\n",
    "        'noise_multiplier': 1.3\n",
    "    }\n",
    "\n",
    "# --- Build the final LSTM+Dense DP model ---\n",
    "model_privacy = tf.keras.Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "for i, units in enumerate(best_hp_dp['lstm_units']):\n",
    "    return_sequences = (i < len(best_hp_dp['lstm_units']) - 1)\n",
    "    model_privacy.add(tf.keras.layers.LSTM(\n",
    "        units=units,\n",
    "        activation=best_hp_dp['activation'],\n",
    "        return_sequences=return_sequences,\n",
    "        input_shape=(1,len(categorical_columns))\n",
    "    ))\n",
    "    model_privacy.add(Dropout(rate=best_hp_dp['dropouts'][i]))\n",
    "\n",
    "# Add Dense layers\n",
    "for j, units in enumerate(best_hp_dp['dense_units']):\n",
    "    model_privacy.add(Dense(units=units, activation=best_hp_dp['activation']))\n",
    "    model_privacy.add(Dropout(rate=best_hp_dp['dropouts'][len(best_hp_dp['lstm_units']) + j]))\n",
    "\n",
    "# Output layer\n",
    "model_privacy.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# DP optimizer\n",
    "dp_optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=best_hp_dp['l2_norm_clip'],\n",
    "    noise_multiplier=best_hp_dp['noise_multiplier'],\n",
    "    num_microbatches=1,\n",
    "    learning_rate=best_hp_dp['learning_rate']\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model_privacy.compile(optimizer=dp_optimizer, loss=best_hp_dp['loss'], metrics=['accuracy'])\n",
    "\n",
    "# --- Cross-validation ---\n",
    "kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train_scaled[categorical_columns], y_train):\n",
    "    X_train_fold, X_val_fold = (\n",
    "        X_train_scaled[categorical_columns].iloc[train_index].values.reshape(-1,1, len(categorical_columns)),\n",
    "        X_train_scaled[categorical_columns].iloc[test_index].values.reshape(-1,1, len(categorical_columns)),\n",
    "    )\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    print(\"hi\")\n",
    "    model_privacy.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=best_hp_dp['epochs'],\n",
    "        batch_size=best_hp_dp['batch_size'],\n",
    "        verbose=0\n",
    "    )\n",
    "    val_loss, val_accuracy = model_privacy.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    cv_results.append([val_loss, val_accuracy])\n",
    "\n",
    "# # --- Final training on full data ---\n",
    "X_train_lstm = X_train_scaled[categorical_columns].values.reshape(-1,1, len(categorical_columns))\n",
    "X_test_lstm = X_test_scaled[categorical_columns].values.reshape(-1,1, len (categorical_columns))\n",
    "\n",
    "# Reshape test data for LSTM;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "# X_test_lstm = X_test_scaled[categorical_columns].values.reshape(-1, 1, len(categorical_columns))\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_test_scaled[categorical_columns] shape:\", X_test_scaled[categorical_columns].values.shape)  # (38378, 46)\n",
    "print(\"X_test_lstm shape:\", X_test_lstm.shape)  # Should be (38378, 1, 46)\n",
    "print(\"Model input shape:\", model_privacy.input_shape)  # (None, 1, 46)\n",
    "\n",
    "# Predict on the reshaped test data\n",
    "y_test_pred_mlp_privacy = model_privacy.predict(X_test_lstm, verbose=0)\n",
    "\n",
    "# Apply threshold for binary predictions\n",
    "y_test_pred_binary_mlp_privacy = y_test_pred_mlp_privacy.argmax(axis=1)  # For softmax/multi-class\n",
    "# OR for binary classification with sigmoid:\n",
    "# y_test_pred_binary_mlp_privacy = (y_test_pred_mlp_privacy > 0.5).astype(int)\n",
    "# ';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "\n",
    "\n",
    "\n",
    "model_history_privacy = model_privacy.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=best_hp_dp['epochs'],\n",
    "    batch_size=best_hp_dp['batch_size'],\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_accuracy = model_privacy.evaluate(X_test_lstm, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNxhAhLW-vYk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvoNwvl5-vYk",
    "outputId": "419c4bbb-5c91-4c78-faee-2fee6a29b207"
   },
   "outputs": [],
   "source": [
    "# confirming reshape;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "print(X_test_scaled[categorical_columns].values.shape)\n",
    "print(len(categorical_columns))\n",
    "print(model_privacy.input_shape)\n",
    "# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BmmS-ATwe3BN",
    "outputId": "194db1d4-9ae3-4e30-b13f-e47483d86341"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "y_test_pred_mlp_privacy = model_privacy.predict(X_test_scaled[categorical_columns].values.reshape(-1,1, len (categorical_columns)))\n",
    "\n",
    "# # Apply the threshold to make binary predictions\n",
    "y_test_pred_binary_mlp_privacy = y_test_pred_mlp_privacy.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mw-sqEJ-e4a3",
    "outputId": "706b2848-40d5-4eda-996a-2f4e50a61766"
   },
   "outputs": [],
   "source": [
    "# Print the average cross-validation results\n",
    "print(\"Average cross-validation loss:\", sum(cv_result[0] for cv_result in cv_results) / len(cv_results))\n",
    "print(\"Average cross-validation accuracy:\", sum(cv_result[1] for cv_result in cv_results) / len(cv_results))\n",
    "\n",
    "# Evaluate the MLP model\n",
    "accuracy_mlp_privacy = accuracy_score(y_test, y_test_pred_binary_mlp_privacy)\n",
    "precision_mlp_privacy = precision_score(y_test, y_test_pred_binary_mlp_privacy, average='macro')\n",
    "recall_mlp_privacy = recall_score(y_test, y_test_pred_binary_mlp_privacy, average='macro')\n",
    "f1_mlp_privacy = f1_score(y_test, y_test_pred_binary_mlp_privacy, average='macro')\n",
    "\n",
    "# Print the results for the MLP model\n",
    "print(\"MLP Model:\")\n",
    "print(f\"Accuracy =  {accuracy_mlp_privacy}\")\n",
    "print(f\"Precision = {precision_mlp_privacy}\")\n",
    "print(f\"Recall = {recall_mlp_privacy}\")\n",
    "print(f\"F1 Score =  {f1_mlp_privacy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XR6-okG8e5kw",
    "outputId": "dcf627e4-719f-419b-fc64-1a55079f574a"
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_loss(model_history_privacy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred_binary_mlp_privacy)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "num_classes = conf_matrix.shape[0]\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "plt.xticks(np.arange(num_classes), class_labels, rotation=45)\n",
    "plt.yticks(np.arange(num_classes), class_labels)\n",
    "\n",
    "thresh = conf_matrix.max() / 2.\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test_binary_true, y_test_pred_binary_mlp_privacy)\n",
    "# area_under_curve = auc(recall, precision)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(recall, precision, label=f'Privacy Model Precision-Recall Curve (AUC = {area_under_curve:.2f})')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Privacy Model Precision-Recall Curve')\n",
    "# plt.legend(loc='lower left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3T3Q5w7e-67"
   },
   "outputs": [],
   "source": [
    "# Extract trainable variables from the model\n",
    "trainable_variables = model_privacy.trainable_variables\n",
    "\n",
    "#correct function\n",
    "privacy_report = compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(\n",
    "    number_of_examples=len(X_train_scaled),\n",
    "    batch_size=32,\n",
    "    noise_multiplier=1.099235020314155,\n",
    "    num_epochs=30,\n",
    "    delta=1e-5\n",
    ")\n",
    "\n",
    "# Compute privacy budget\n",
    "#privacy_report = compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(\n",
    "   # number_of_examples=len(X_train_scaled),\n",
    "   # batch_size=batch_size,\n",
    "   # noise_multiplier=noise_multiplier,\n",
    "   # num_epochs=3,\n",
    "   # delta=1e-5\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz7Y3IDlfA3Z",
    "outputId": "17fad0e3-8f60-4039-cd60-7525da534a3c"
   },
   "outputs": [],
   "source": [
    "print(privacy_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSE7doofZ-hV"
   },
   "source": [
    "## Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "QUjFgv3pZ_j2",
    "outputId": "3b1891d5-8b8d-4800-ccb5-0fb1e26c8047"
   },
   "outputs": [],
   "source": [
    "# # Create DataFrame\n",
    "# data = {\n",
    "#     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "#     'Regular': [accuracy_lstm, precision_lstm, recall_lstm, f1_lstm],\n",
    "#     'Privacy': [accuracy_mlp_privacy, precision_mlp_privacy, recall_mlp_privacy, f1_mlp_privacy]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Print DataFrame\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "R8blG2_ldgzC",
    "outputId": "c6d9b151-ef9a-4b7b-e64c-48ab0500f989"
   },
   "outputs": [],
   "source": [
    "# plot_loss(history, model_history_privacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_CuDgcKeb2K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khz5KxcW-vYs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
